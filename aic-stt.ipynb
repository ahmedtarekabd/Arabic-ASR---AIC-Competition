{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8840294,"sourceType":"datasetVersion","datasetId":5320319},{"sourceId":8842661,"sourceType":"datasetVersion","datasetId":5322027}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# %pip install torch torchaudio scikit-learn pandas matplotlib numpy seaborn","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:20.145378Z","iopub.execute_input":"2024-07-03T10:13:20.145796Z","iopub.status.idle":"2024-07-03T10:13:20.150844Z","shell.execute_reply.started":"2024-07-03T10:13:20.145762Z","shell.execute_reply":"2024-07-03T10:13:20.149781Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"print(\"Hello World!\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:20.152188Z","iopub.execute_input":"2024-07-03T10:13:20.152493Z","iopub.status.idle":"2024-07-03T10:13:20.164648Z","shell.execute_reply.started":"2024-07-03T10:13:20.152463Z","shell.execute_reply":"2024-07-03T10:13:20.163673Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Hello World!\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom collections import Counter\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ModelCheckpoint\n\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:20.166487Z","iopub.execute_input":"2024-07-03T10:13:20.166962Z","iopub.status.idle":"2024-07-03T10:13:26.089243Z","shell.execute_reply.started":"2024-07-03T10:13:20.166929Z","shell.execute_reply":"2024-07-03T10:13:26.088237Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-07-03 10:13:23.305440: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-03 10:13:23.305502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-03 10:13:23.306957: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Source: https://www.kaggle.com/discussions/getting-started/140636\n# !pip install GPUtil\n\n# import torch\n# from GPUtil import showUtilization as gpu_usage\n# from numba import cuda\n\n# def free_gpu_cache():\n#     print(\"Initial GPU Usage\")\n#     gpu_usage()                             \n\n#     torch.cuda.empty_cache()\n\n#     cuda.select_device(0)\n#     cuda.close()\n#     cuda.select_device(0)\n\n#     print(\"GPU Usage after emptying the cache\")\n#     gpu_usage()\n\n# if torch.cuda.is_available():\n#     free_gpu_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.090559Z","iopub.execute_input":"2024-07-03T10:13:26.091223Z","iopub.status.idle":"2024-07-03T10:13:26.095812Z","shell.execute_reply.started":"2024-07-03T10:13:26.091195Z","shell.execute_reply":"2024-07-03T10:13:26.094773Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(str(torchaudio.list_audio_backends()))\n# %pip install PySoundFile","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.098227Z","iopub.execute_input":"2024-07-03T10:13:26.098574Z","iopub.status.idle":"2024-07-03T10:13:26.110139Z","shell.execute_reply.started":"2024-07-03T10:13:26.098544Z","shell.execute_reply":"2024-07-03T10:13:26.109235Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['ffmpeg', 'soundfile']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(torch.cuda.is_available())  # Should return True if CUDA is available","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.111180Z","iopub.execute_input":"2024-07-03T10:13:26.111454Z","iopub.status.idle":"2024-07-03T10:13:26.152928Z","shell.execute_reply.started":"2024-07-03T10:13:26.111431Z","shell.execute_reply":"2024-07-03T10:13:26.151862Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"## kaggle Setup","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.154260Z","iopub.execute_input":"2024-07-03T10:13:26.154580Z","iopub.status.idle":"2024-07-03T10:13:26.164857Z","shell.execute_reply.started":"2024-07-03T10:13:26.154546Z","shell.execute_reply":"2024-07-03T10:13:26.164085Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/input/arabic-asr/\"\nTRAIN_PATH = DATASET_PATH + \"train/\"\nADAPT_PATH = DATASET_PATH + \"adapt/\"\nbatch_size = 16","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.165923Z","iopub.execute_input":"2024-07-03T10:13:26.166185Z","iopub.status.idle":"2024-07-03T10:13:26.174793Z","shell.execute_reply.started":"2024-07-03T10:13:26.166163Z","shell.execute_reply":"2024-07-03T10:13:26.173934Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"# Load the CSV file\ndata = pd.read_csv(DATASET_PATH + 'train.csv') # nrows=1000\n# Display the first few rows of the dataset\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.175960Z","iopub.execute_input":"2024-07-03T10:13:26.176195Z","iopub.status.idle":"2024-07-03T10:13:26.380109Z","shell.execute_reply.started":"2024-07-03T10:13:26.176174Z","shell.execute_reply":"2024-07-03T10:13:26.379205Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"            audio                                         transcript\n0  train_sample_0  على إنها عار في الوقت اللي كانت بتتعامل مع أخو...\n1  train_sample_1  فأكيد ربنا عوضهم خير هو الراجل بيبقى ليه إختيا...\n2  train_sample_2  زي دول كتيره بنشوفها النهارده في العالم وأصبحت...\n3  train_sample_3  يعني مين اللي بيحط شروطها يعني أنا شايفه إني م...\n4  train_sample_4  والله هي الموضوع مش كليب خلي بالك ولا أغنيه ال...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio</th>\n      <th>transcript</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_sample_0</td>\n      <td>على إنها عار في الوقت اللي كانت بتتعامل مع أخو...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_sample_1</td>\n      <td>فأكيد ربنا عوضهم خير هو الراجل بيبقى ليه إختيا...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_sample_2</td>\n      <td>زي دول كتيره بنشوفها النهارده في العالم وأصبحت...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_sample_3</td>\n      <td>يعني مين اللي بيحط شروطها يعني أنا شايفه إني م...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_sample_4</td>\n      <td>والله هي الموضوع مش كليب خلي بالك ولا أغنيه ال...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.381185Z","iopub.execute_input":"2024-07-03T10:13:26.381444Z","iopub.status.idle":"2024-07-03T10:13:26.403405Z","shell.execute_reply.started":"2024-07-03T10:13:26.381422Z","shell.execute_reply":"2024-07-03T10:13:26.402598Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50715 entries, 0 to 50714\nData columns (total 2 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   audio       50715 non-null  object\n 1   transcript  50709 non-null  object\ndtypes: object(2)\nmemory usage: 792.5+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.407373Z","iopub.execute_input":"2024-07-03T10:13:26.407893Z","iopub.status.idle":"2024-07-03T10:13:26.426944Z","shell.execute_reply.started":"2024-07-03T10:13:26.407867Z","shell.execute_reply":"2024-07-03T10:13:26.426019Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"audio         0\ntranscript    6\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data.dropna(inplace=True)\ndata.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.428045Z","iopub.execute_input":"2024-07-03T10:13:26.428323Z","iopub.status.idle":"2024-07-03T10:13:26.463288Z","shell.execute_reply.started":"2024-07-03T10:13:26.428290Z","shell.execute_reply":"2024-07-03T10:13:26.462360Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"audio         0\ntranscript    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data.transcript.str.contains(r'[<.*>|=|÷|??]', na=False).sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.464447Z","iopub.execute_input":"2024-07-03T10:13:26.464790Z","iopub.status.idle":"2024-07-03T10:13:26.554035Z","shell.execute_reply.started":"2024-07-03T10:13:26.464759Z","shell.execute_reply":"2024-07-03T10:13:26.553080Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"418"},"metadata":{}}]},{"cell_type":"code","source":"data[data.transcript.str.contains(r'<.*>', na=False)]","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.555004Z","iopub.execute_input":"2024-07-03T10:13:26.555258Z","iopub.status.idle":"2024-07-03T10:13:26.592574Z","shell.execute_reply.started":"2024-07-03T10:13:26.555236Z","shell.execute_reply":"2024-07-03T10:13:26.591615Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                    audio                                         transcript\n786      train_sample_786  <fil> هو مختارش إن هو يتكلم عن نفسه كأوباما ول...\n3690    train_sample_3690  <fil> الانتخابات اللي فاتت مش السابقه دي الأست...\n19449  train_sample_19449  <fil> ساعة ونس بنتونس مع بعض يعني <fil> الكلام...\n19463  train_sample_19463      <fil> <fil> <fil> <fil> <fil> <fil> <overlap>\n19613  train_sample_19613  <fil> مفيش جنيه <fil> اشتغلت مع عمي في المنيا ...\n...                   ...                                                ...\n50211  train_sample_50211  و لوحدي ما شاء الله لا لوحدي <fil> طب بابا مكل...\n50224  train_sample_50224                         هو <fil> ملوش أمانه بتتولى\n50254  train_sample_50254  فكرة إن الدول دي تستفيد من <fil> هو يمكن هطلع ...\n50271  train_sample_50271  فمنقدرش نربط الشهاده <fil> أكتر من سنه <fil> ت...\n50299  train_sample_50299  يعني مش <fil> خلاص بقى يسيبوا بقى مين دول و لا...\n\n[412 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio</th>\n      <th>transcript</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>786</th>\n      <td>train_sample_786</td>\n      <td>&lt;fil&gt; هو مختارش إن هو يتكلم عن نفسه كأوباما ول...</td>\n    </tr>\n    <tr>\n      <th>3690</th>\n      <td>train_sample_3690</td>\n      <td>&lt;fil&gt; الانتخابات اللي فاتت مش السابقه دي الأست...</td>\n    </tr>\n    <tr>\n      <th>19449</th>\n      <td>train_sample_19449</td>\n      <td>&lt;fil&gt; ساعة ونس بنتونس مع بعض يعني &lt;fil&gt; الكلام...</td>\n    </tr>\n    <tr>\n      <th>19463</th>\n      <td>train_sample_19463</td>\n      <td>&lt;fil&gt; &lt;fil&gt; &lt;fil&gt; &lt;fil&gt; &lt;fil&gt; &lt;fil&gt; &lt;overlap&gt;</td>\n    </tr>\n    <tr>\n      <th>19613</th>\n      <td>train_sample_19613</td>\n      <td>&lt;fil&gt; مفيش جنيه &lt;fil&gt; اشتغلت مع عمي في المنيا ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>50211</th>\n      <td>train_sample_50211</td>\n      <td>و لوحدي ما شاء الله لا لوحدي &lt;fil&gt; طب بابا مكل...</td>\n    </tr>\n    <tr>\n      <th>50224</th>\n      <td>train_sample_50224</td>\n      <td>هو &lt;fil&gt; ملوش أمانه بتتولى</td>\n    </tr>\n    <tr>\n      <th>50254</th>\n      <td>train_sample_50254</td>\n      <td>فكرة إن الدول دي تستفيد من &lt;fil&gt; هو يمكن هطلع ...</td>\n    </tr>\n    <tr>\n      <th>50271</th>\n      <td>train_sample_50271</td>\n      <td>فمنقدرش نربط الشهاده &lt;fil&gt; أكتر من سنه &lt;fil&gt; ت...</td>\n    </tr>\n    <tr>\n      <th>50299</th>\n      <td>train_sample_50299</td>\n      <td>يعني مش &lt;fil&gt; خلاص بقى يسيبوا بقى مين دول و لا...</td>\n    </tr>\n  </tbody>\n</table>\n<p>412 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.drop(data[data.transcript.str.contains(r'<.*>', na=False)].index, inplace=True)\ndata.transcript.str.contains(r'<.*>', na=False).sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.593834Z","iopub.execute_input":"2024-07-03T10:13:26.594685Z","iopub.status.idle":"2024-07-03T10:13:26.659618Z","shell.execute_reply.started":"2024-07-03T10:13:26.594653Z","shell.execute_reply":"2024-07-03T10:13:26.658716Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"data['audio'] = TRAIN_PATH + data['audio'] + \".wav\"\ndata.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.660893Z","iopub.execute_input":"2024-07-03T10:13:26.661327Z","iopub.status.idle":"2024-07-03T10:13:26.687069Z","shell.execute_reply.started":"2024-07-03T10:13:26.661295Z","shell.execute_reply":"2024-07-03T10:13:26.686039Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data.iloc[784:787]","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.688260Z","iopub.execute_input":"2024-07-03T10:13:26.688553Z","iopub.status.idle":"2024-07-03T10:13:26.699424Z","shell.execute_reply.started":"2024-07-03T10:13:26.688529Z","shell.execute_reply":"2024-07-03T10:13:26.698510Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                                 audio  \\\n784  /kaggle/input/arabic-asr/train/train_sample_78...   \n785  /kaggle/input/arabic-asr/train/train_sample_78...   \n786  /kaggle/input/arabic-asr/train/train_sample_78...   \n\n                                            transcript  \n784      لواء أركان حرب أيمن حب الدين نكمل إن شاء الله  \n785  والأم هنيجي لرساله للأب والأم هنا إرحموا ولادك...  \n786  و خاصة الدول الناميه ومنها مصر بتعاني النهارده...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio</th>\n      <th>transcript</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>784</th>\n      <td>/kaggle/input/arabic-asr/train/train_sample_78...</td>\n      <td>لواء أركان حرب أيمن حب الدين نكمل إن شاء الله</td>\n    </tr>\n    <tr>\n      <th>785</th>\n      <td>/kaggle/input/arabic-asr/train/train_sample_78...</td>\n      <td>والأم هنيجي لرساله للأب والأم هنا إرحموا ولادك...</td>\n    </tr>\n    <tr>\n      <th>786</th>\n      <td>/kaggle/input/arabic-asr/train/train_sample_78...</td>\n      <td>و خاصة الدول الناميه ومنها مصر بتعاني النهارده...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Define Speech Dataset Class","metadata":{}},{"cell_type":"code","source":"class SpeechDataset(Dataset):\n    def __init__(self, data: str | pd.DataFrame, n_mels=40, sample_rate=16000):\n        if isinstance(data, str):\n            self.data = pd.read_csv(data)\n        else:\n            self.data = data\n        self.sample_rate = sample_rate\n        self.n_mels = n_mels\n        self.vocab = self._build_vocab(self.data['transcript'].tolist())\n\n    def _build_vocab(self, transcripts):\n        all_chars = ''.join(transcripts)\n        char_counter = Counter(all_chars)\n        vocab = {char: idx + 1 for idx, (char, _) in enumerate(char_counter.most_common())}\n        vocab['<pad>'] = 0  # Padding token\n        return vocab\n\n    \"\"\"\n        * If you are certain that your audio data is already sampled at 16kHz, \n        then you do not need to perform the resampling step.\n        * The purpose of the resampling step in the provided code is to ensure that\n        the audio waveform is at the desired sample rate (in this case, 16kHz).\n    \"\"\"\n    def _load_audio(self, file_path, resample: bool = True):\n        \"\"\"\n            Load an audio file and resample it to the desired sample rate.\n            file_path: Path to the audio file.\n            resample: Whether to resample the audio to the desired sample rate.\n            sample_rate: Desired sample rate.\n            Returns: Audio waveform tensor.\n        \"\"\"\n        # Load audio waveform and sample rate\n        waveform, sr = torchaudio.load(file_path)\n        if resample and sr != self.sample_rate:\n            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)  # Create a resampler object\n            waveform = resampler(waveform)  # Resample the waveform to the desired sample rate\n        return waveform.squeeze()  # Remove extra dimension\n\n    def _extract_features(self, audio):\n        transform = torchaudio.transforms.MelSpectrogram(sample_rate=self.sample_rate, n_mels=self.n_mels)\n        mel_spectrogram = transform(audio)\n        mel_spectrogram = torch.log(mel_spectrogram + 1e-9)  # Log-scale for numerical stability\n        return mel_spectrogram.T  # Transpose to have time steps as rows\n\n    def _text_to_sequence(self, text):\n        return [self.vocab[char] for char in text]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        try:\n            audio_path = self.data['audio'][idx]\n            transcript = self.data['transcript'][idx]\n        \n            audio = self._load_audio(audio_path)\n            mel_features = self._extract_features(audio)\n            target = self._text_to_sequence(transcript)\n            # print(mel_features.shape, len(target))\n        except Exception as e:\n            print(f\"Error processing {idx}\")\n            return self.__getitem__(idx + 1)\n        return mel_features, target\n\n\n    def collate_fn(self, batch):\n        inputs, targets = zip(*batch)\n        \n        inputs_padded = pad_sequence([torch.tensor(inp, dtype=torch.float32) for inp in inputs], batch_first=True)\n        targets_padded = pad_sequence([torch.tensor(tgt, dtype=torch.long) for tgt in targets], batch_first=True)\n        return inputs_padded, targets_padded\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.700883Z","iopub.execute_input":"2024-07-03T10:13:26.701258Z","iopub.status.idle":"2024-07-03T10:13:26.719178Z","shell.execute_reply.started":"2024-07-03T10:13:26.701229Z","shell.execute_reply":"2024-07-03T10:13:26.718264Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Path to your train.csv\ncsv_path = DATASET_PATH + 'train.csv'\n\nfeature_dim = 60\n\n# Instantiate the dataset\nspeech_dataset = SpeechDataset(data, n_mels=feature_dim)\n\n# Create DataLoader\ntrain_loader = DataLoader(speech_dataset, batch_size=batch_size, shuffle=False, collate_fn=speech_dataset.collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:26.720432Z","iopub.execute_input":"2024-07-03T10:13:26.720775Z","iopub.status.idle":"2024-07-03T10:13:27.111076Z","shell.execute_reply.started":"2024-07-03T10:13:26.720746Z","shell.execute_reply":"2024-07-03T10:13:27.110103Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# speech_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:27.112343Z","iopub.execute_input":"2024-07-03T10:13:27.112978Z","iopub.status.idle":"2024-07-03T10:13:27.122811Z","shell.execute_reply.started":"2024-07-03T10:13:27.112940Z","shell.execute_reply":"2024-07-03T10:13:27.121847Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## Listener","metadata":{}},{"cell_type":"code","source":"# Define the Listener (Encoder) with Pyramidal BLSTM\nclass Listener(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers):\n        \"\"\"\n            Initialize the Listener with a Pyramidal BLSTM.\n            input_dim: Dimension of input features (e.g., size of the MFCC or Mel-spectrogram features).\n            hidden_dim: Number of hidden units in each LSTM layer.\n            num_layers: Number of LSTM layers.\n        \"\"\"\n        super(Listener, self).__init__()\n        self.layers = nn.ModuleList()  # List to hold the LSTM layers.\n        current_dim = input_dim  # Start with the input dimension.\n        \n        for _ in range(num_layers):\n            # Append a bidirectional LSTM layer to the list.\n            self.layers.append(nn.LSTM(current_dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=True))\n            # Update the current dimension to match the BLSTM output.\n            current_dim = hidden_dim * 2  # BLSTM doubles the hidden dimension.\n\n    def forward(self, x):\n        \"\"\"\n            Forward pass through the Listener.\n            x: Input tensor of shape (batch_size, sequence_length, input_dim).\n            Returns: Downsampled sequence of encoded features.\n        \"\"\"\n        for lstm in self.layers:\n            x, _ = lstm(x)  # Pass the input through the current LSTM layer.\n\n#             # Print shape information for debugging.\n#             print(f'Before downsampling: batch_size={x.size(0)}, seq_len={x.size(1)}, feature_dim={x.size(2)}')\n            \n            # Downsample by concatenating adjacent time steps.\n            batch_size, seq_len, feature_dim = x.shape\n\n#             # Print shape information for debugging.\n#             print(f'After downsampling: batch_size={x.size(0)}, seq_len={x.size(1)}, feature_dim={x.size(2)}')\n\n            # x.contiguous(): This ensures that the tensor's memory layout is contiguous\n            # view(batch_size, seq_len // 2, feature_dim * 2): The view function reshapes the tensor. \n            # x = x.contiguous().view(batch_size, seq_len // 2, feature_dim * 2)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:27.123949Z","iopub.execute_input":"2024-07-03T10:13:27.124237Z","iopub.status.idle":"2024-07-03T10:13:27.135399Z","shell.execute_reply.started":"2024-07-03T10:13:27.124206Z","shell.execute_reply":"2024-07-03T10:13:27.134558Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Attender","metadata":{}},{"cell_type":"code","source":"# Define the Attender with Attention Context\nclass Attender(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        \"\"\"\n        Initialize the Attender with an attention mechanism.\n        encoder_dim: Dimension of the encoded features from the Listener.\n        decoder_dim: Dimension of the hidden state from the Speller.\n        attention_dim: Dimension of the attention layer.\n        \"\"\"\n        super(Attender, self).__init__()\n        # Linear layer to transform the encoder and decoder states into the attention space.\n        self.attn = nn.Linear(encoder_dim + decoder_dim, attention_dim)\n        # Linear layer to compute the attention energies.\n        self.v = nn.Linear(attention_dim, 1, bias=False)\n\n    def forward(self, encoder_outputs, decoder_hidden):\n        \"\"\"\n        Forward pass through the Attender.\n        encoder_outputs: Encoded features from the Listener (batch_size, sequence_length, encoder_dim).\n        decoder_hidden: Hidden state from the Speller (batch_size, decoder_dim).\n        Returns: Context vector and attention weights.\n        \"\"\"\n        # Calculate attention energies by concatenating encoder outputs with the decoder hidden state.\n        seq_len = encoder_outputs.size(1)\n        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n        energy = torch.cat((encoder_outputs, decoder_hidden), dim=2)\n        attn_energies = self.v(F.tanh(self.attn(energy))).squeeze(2)\n        # Apply softmax to get attention weights.\n        attn_weights = F.softmax(attn_energies, dim=1)\n        # Compute the context vector as a weighted sum of encoder outputs.\n        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n        return context, attn_weights","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:27.136494Z","iopub.execute_input":"2024-07-03T10:13:27.136780Z","iopub.status.idle":"2024-07-03T10:13:27.149880Z","shell.execute_reply.started":"2024-07-03T10:13:27.136756Z","shell.execute_reply":"2024-07-03T10:13:27.149090Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Speller","metadata":{}},{"cell_type":"code","source":"# Define the Speller (Decoder) with Character-Level Model\nclass Speller(nn.Module):\n    def __init__(self, vocab_size, encoder_dim, hidden_dim, num_layers):\n        \"\"\"\n            Initialize the Speller.\n            vocab_size: Size of the output vocabulary (number of possible output tokens)\n            hidden_dim: Number of hidden units in the LSTM layers\n            num_layers: Number of LSTM layers\n        \"\"\"\n        super(Speller, self).__init__()\n        self.lstm = nn.LSTM(encoder_dim + hidden_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n\n    def forward(self, context, hidden, cell, previous_char):\n        \"\"\"\n            Forward pass through the Speller.\n            context: Context vector from the Attender (batch_size, hidden_dim * 2)\n            hidden: Initial hidden state for the LSTM\n            cell: Initial cell state for the LSTM\n            Returns: Output token probabilities (batch_size, vocab_size), Updated hidden state, Updated cell state\n        \"\"\"\n\n        # Embed the previous character\n        embedded = self.embedding(previous_char).unsqueeze(1)\n        # Concatenate context and embedded character\n        lstm_input = torch.cat((context.unsqueeze(1), embedded), dim=2)\n        # Pass through LSTM\n        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n        # Map to vocabulary size\n        output = self.fc(output.squeeze(1))\n        return output, hidden, cell","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:27.150986Z","iopub.execute_input":"2024-07-03T10:13:27.151252Z","iopub.status.idle":"2024-07-03T10:13:27.164683Z","shell.execute_reply.started":"2024-07-03T10:13:27.151230Z","shell.execute_reply":"2024-07-03T10:13:27.163851Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## LAS Model","metadata":{}},{"cell_type":"code","source":"class LASModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, vocab_size, num_layers):\n        \"\"\"\n        Initialize the LAS model.\n        input_dim: Dimension of input features (e.g., MFCC features).\n        hidden_dim: Number of hidden units in each LSTM layer.\n        vocab_size: Size of the output vocabulary (number of grapheme characters).\n        num_layers: Number of LSTM layers.\n        \"\"\"\n        super(LASModel, self).__init__()\n        self.listener = Listener(input_dim, hidden_dim, num_layers)\n        self.attender = Attender(hidden_dim * 2, hidden_dim, hidden_dim)\n        self.speller = Speller(vocab_size, hidden_dim * 2, hidden_dim, num_layers)\n        self.vocab_size = vocab_size\n\n    def forward(self, inputs, targets, teacher_forcing_ratio=0.5):\n        \"\"\"\n        Forward pass through the LAS model.\n        inputs: Input features (batch_size, sequence_length, input_dim).\n        targets: Target character sequence (batch_size, target_length).\n        teacher_forcing_ratio: Probability of using true target instead of predicted character during training.\n        \"\"\"\n        encoder_outputs = self.listener(inputs)\n        batch_size = targets.size(0)\n        target_length = targets.size(1)\n        outputs = torch.zeros(batch_size, target_length, self.vocab_size).to(inputs.device)\n\n        hidden = torch.zeros(self.speller.lstm.num_layers, batch_size, self.speller.lstm.hidden_size).to(inputs.device)\n        cell = torch.zeros(self.speller.lstm.num_layers, batch_size, self.speller.lstm.hidden_size).to(inputs.device)\n        previous_char = targets[:, 0]\n\n        for t in range(1, target_length):\n            context, _ = self.attender(encoder_outputs, hidden[-1])\n            output, hidden, cell = self.speller(context, hidden, cell, previous_char)\n            outputs[:, t, :] = output\n\n            if torch.rand(1).item() < teacher_forcing_ratio:\n                previous_char = targets[:, t]\n            else:\n                previous_char = output.argmax(1)\n\n        return outputs\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:27.165965Z","iopub.execute_input":"2024-07-03T10:13:27.166312Z","iopub.status.idle":"2024-07-03T10:13:27.178673Z","shell.execute_reply.started":"2024-07-03T10:13:27.166289Z","shell.execute_reply":"2024-07-03T10:13:27.177750Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Audio Preprocessing","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    * If you are certain that your audio data is already sampled at 16kHz, \n    then you do not need to perform the resampling step.\n    * The purpose of the resampling step in the provided code is to ensure that\n    the audio waveform is at the desired sample rate (in this case, 16kHz).\n\"\"\"\ndef load_audio(file_path, resample: bool = True, sample_rate=16000):\n    \"\"\"\n        Load an audio file and resample it to the desired sample rate.\n        file_path: Path to the audio file.\n        resample: Whether to resample the audio to the desired sample rate.\n        sample_rate: Desired sample rate.\n        Returns: Audio waveform tensor.\n    \"\"\"\n    # Load audio waveform and sample rate\n    waveform, sr = torchaudio.load(file_path)\n    if resample and sr != sample_rate:\n        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)  # Create a resampler object\n        waveform = resampler(waveform)  # Resample the waveform to the desired sample rate\n    return waveform.squeeze()  # Remove extra dimension","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:27.179922Z","iopub.execute_input":"2024-07-03T10:13:27.180223Z","iopub.status.idle":"2024-07-03T10:13:27.192182Z","shell.execute_reply.started":"2024-07-03T10:13:27.180188Z","shell.execute_reply":"2024-07-03T10:13:27.191298Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def extract_features(audio, sample_rate=16000, n_mels=40):\n    \"\"\"\n        Extract Mel spectrogram features from an audio waveform.\n        audio: Input audio waveform.\n        sample_rate: Sample rate of the audio waveform.\n        n_mels: Number of Mel filterbanks.\n        Returns: Mel spectrogram tensor.\n    \"\"\"\n    transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)\n    mel_spectrogram = transform(audio)\n    mel_spectrogram = torch.log(mel_spectrogram + 1e-9)  # Log-scale for numerical stability\n    return mel_spectrogram.T  # Transpose to have time steps as rows","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:27.193394Z","iopub.execute_input":"2024-07-03T10:13:27.193996Z","iopub.status.idle":"2024-07-03T10:13:27.206319Z","shell.execute_reply.started":"2024-07-03T10:13:27.193966Z","shell.execute_reply":"2024-07-03T10:13:27.205605Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# # Example usage\n# audio_path = data['audio'][0]\n# audio = load_audio(audio_path)\n# mel_features = extract_features(audio)\n# print(mel_features.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:27.207281Z","iopub.execute_input":"2024-07-03T10:13:27.207534Z","iopub.status.idle":"2024-07-03T10:13:27.215577Z","shell.execute_reply.started":"2024-07-03T10:13:27.207512Z","shell.execute_reply":"2024-07-03T10:13:27.214760Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Text Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Tokenization, vectorization, and sequencing","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\ndef build_vocab(transcripts):\n    all_chars = ''.join(transcripts)\n    char_counter = Counter(all_chars)\n    vocab = {char: idx + 1 for idx, (char, _) in enumerate(char_counter.most_common())}\n    vocab['<pad>'] = 0  # Add padding token\n    return vocab\n\n# Build vocabulary\nvocab = build_vocab(data['transcript'])\nprint(vocab)\n\ndef text_to_sequence(text, vocab):\n    return [vocab[char] for char in text]\n\n# Example usage\ntranscript = data['transcript'][0]\nsequence = text_to_sequence(transcript, vocab)\nprint(sequence)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:27.216746Z","iopub.execute_input":"2024-07-03T10:13:27.217092Z","iopub.status.idle":"2024-07-03T10:13:27.635213Z","shell.execute_reply.started":"2024-07-03T10:13:27.217063Z","shell.execute_reply":"2024-07-03T10:13:27.634051Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"{' ': 1, 'ا': 2, 'ل': 3, 'ي': 4, 'ن': 5, 'م': 6, 'ه': 7, 'و': 8, 'ت': 9, 'ب': 10, 'ع': 11, 'ر': 12, 'د': 13, 'ك': 14, 'ف': 15, 'أ': 16, 'س': 17, 'ح': 18, 'ق': 19, 'ش': 20, 'إ': 21, 'ج': 22, 'ص': 23, 'ط': 24, 'خ': 25, 'ى': 26, 'ز': 27, 'ض': 28, 'ة': 29, 'غ': 30, 'ذ': 31, 'ث': 32, 'ء': 33, 'ظ': 34, 'ئ': 35, 'ؤ': 36, 'آ': 37, 'ً': 38, 'ڨ': 39, '<': 40, '÷': 41, '،': 42, '>': 43, 'ِ': 44, '⁇': 45, 'n': 46, '=': 47, '١': 48, 'ٱ': 49, 'چ': 50, '[': 51, '<pad>': 0}\n[11, 3, 26, 1, 21, 5, 7, 2, 1, 11, 2, 12, 1, 15, 4, 1, 2, 3, 8, 19, 9, 1, 2, 3, 3, 4, 1, 14, 2, 5, 9, 1, 10, 9, 9, 11, 2, 6, 3, 1, 6, 11, 1, 16, 25, 8, 4, 2, 1, 2, 3, 8, 3, 13, 1, 2, 3, 8, 18, 4, 13, 1, 6, 11, 2, 6, 3, 7, 1, 25, 2, 23, 7]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"markdown","source":"## Initalize Model","metadata":{}},{"cell_type":"code","source":"# Parameters\ninput_dim = feature_dim  # Mel-spectrogram dimension\nhidden_dim = 128  # Hidden layer size\nvocab_size = len(speech_dataset.vocab)  # Vocabulary size\nnum_layers = 10  # Number of LSTM layers\nlearning_rate = 0.001  # Learning rate\nnum_epochs = 30  # Number of epochs\n\nprint(vocab_size)\n\n# Instantiate the model\nif torch.cuda.is_available():\n    model = LASModel(input_dim, hidden_dim, vocab_size, num_layers, ).to('cuda')\nelse:\n    model = LASModel(input_dim, hidden_dim, vocab_size, num_layers).to('cpu')\ncriterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index in loss calculation\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:27.640164Z","iopub.execute_input":"2024-07-03T10:13:27.640480Z","iopub.status.idle":"2024-07-03T10:13:28.831725Z","shell.execute_reply.started":"2024-07-03T10:13:27.640446Z","shell.execute_reply":"2024-07-03T10:13:28.830933Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"52\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Parameters\n# input_dim = 40  # Example input dimension for MFCC features.\n# hidden_dim = 256  # Hidden dimension size.\n# vocab_size = 100  # Size of the character vocabulary.\n# num_layers = 3  # Number of LSTM layers.\n# learning_rate = 0.001  # Learning rate.\n# num_epochs = 20  # Number of epochs to train.\n# teacher_forcing_ratio = 0.5  # Teacher forcing ratio.\n\n# # Define the model, loss function, and optimizer\n# model = LASModel(input_dim, hidden_dim, vocab_size, num_layers).to('cuda')\n# criterion = nn.CrossEntropyLoss(ignore_index=0)  # Use an appropriate ignore index for padding.\n# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# # Example data loader (replace with your own data loader)\n# def get_dataloader(batch_size):\n#     # Return a PyTorch DataLoader with your training data\n#     pass  # Implement your data loading here\n\n# train_loader = get_dataloader(batch_size=32)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:28.832723Z","iopub.execute_input":"2024-07-03T10:13:28.833011Z","iopub.status.idle":"2024-07-03T10:13:28.837705Z","shell.execute_reply.started":"2024-07-03T10:13:28.832987Z","shell.execute_reply":"2024-07-03T10:13:28.836719Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Checkpoints","metadata":{}},{"cell_type":"markdown","source":"#### Saving Checkpoints","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, epoch, loss, best_loss, checkpoint_dir='checkpoints', filename='best_model.pth'):\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n        \n    checkpoint_path = os.path.join(checkpoint_dir, filename)\n    # Save the model if current loss is the best\n    if loss < best_loss:\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n        }, checkpoint_path)\n        print(f'Checkpoint saved at epoch {epoch + 1} with loss {loss:.4f}')\n        best_loss = loss  # Update best loss\n    return best_loss","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:28.838840Z","iopub.execute_input":"2024-07-03T10:13:28.839128Z","iopub.status.idle":"2024-07-03T10:13:28.853376Z","shell.execute_reply.started":"2024-07-03T10:13:28.839105Z","shell.execute_reply":"2024-07-03T10:13:28.852565Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"#### Load Checkpoints","metadata":{}},{"cell_type":"code","source":"def load_checkpoint(checkpoint_path, model, optimizer):\n    if not os.path.exists(checkpoint_path):\n        print(f\"No checkpoint found at '{checkpoint_path}'\")\n        return model, optimizer, -1, float('inf')\n\n    try:\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        epoch = checkpoint['epoch']\n        best_loss = checkpoint['loss']\n        print(f\"Loaded checkpoint from '{checkpoint_path}' (epoch {epoch + 1}, loss {best_loss:.4f})\")\n    except RuntimeError as e:\n#         print(f\"Error loading checkpoint: {e}\")\n#         print(\"Mismatch in model architecture. Check the dimensions of your model layers.\")\n#         print(\"Attempting to load state dict with strict=False.\")\n        \n#         try:\n#             # Load model with strict=False to ignore size mismatches\n#             model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n#             print(\"Checkpoint loaded with some mismatches in layer sizes.\")\n#         except Exception as e2:\n#             print(f\"Error loading with strict=False: {e2}\")\n#             print(\"Could not load the model checkpoint. Please ensure the model architecture matches the checkpoint.\")\n        return model, optimizer, -1, float('inf')\n        \n    return model, optimizer, checkpoint.get('epoch', 0), checkpoint.get('loss', float('inf'))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:13:28.854560Z","iopub.execute_input":"2024-07-03T10:13:28.854926Z","iopub.status.idle":"2024-07-03T10:13:28.864740Z","shell.execute_reply.started":"2024-07-03T10:13:28.854896Z","shell.execute_reply":"2024-07-03T10:13:28.863921Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Train Loop","metadata":{}},{"cell_type":"code","source":"###### free_gpu_cache()\n\n# Path to the checkpoint file\ncheckpoint_path = 'checkpoints/best_model.pth'\n# Load the checkpoint if it exists\nmodel, optimizer, start_epoch, best_loss = load_checkpoint(checkpoint_path, model, optimizer)\nprint(start_epoch)\nfor epoch in range(start_epoch + 1, num_epochs):\n    model.train()\n    epoch_loss = 0\n    \n    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n        for inputs, targets in train_loader:\n            if torch.cuda.is_available():\n                inputs, targets = inputs.to('cuda'), targets.to('cuda')\n            else:\n                inputs, targets = inputs.to('cpu'), targets.to('cpu')\n\n#             print(inputs.shape, targets.shape)\n\n            optimizer.zero_grad()\n            outputs = model(inputs, targets)\n\n            outputs = outputs[:, 1:].reshape(-1, vocab_size)\n            targets = targets[:, 1:].reshape(-1)\n\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            pbar.set_postfix(loss=epoch_loss / (pbar.n + 1))  # Display average loss\n            pbar.update(1)\n\n    best_loss = save_checkpoint(model, optimizer, epoch, epoch_loss, best_loss, checkpoint_dir='checkpoints', filename='best_model.pth')\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:14:31.857507Z","iopub.execute_input":"2024-07-03T10:14:31.858101Z","iopub.status.idle":"2024-07-03T19:28:08.166791Z","shell.execute_reply.started":"2024-07-03T10:14:31.858060Z","shell.execute_reply":"2024-07-03T19:28:08.164962Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"-1\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/30:   0%|          | 0/3144 [00:00<?, ?batch/s]/tmp/ipykernel_337/1910466668.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  inputs_padded = pad_sequence([torch.tensor(inp, dtype=torch.float32) for inp in inputs], batch_first=True)\nEpoch 1/30: 100%|██████████| 3144/3144 [52:43<00:00,  1.01s/batch, loss=2.97]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 1 with loss 9325.6330\nEpoch [1/30], Loss: 2.9662\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30: 100%|██████████| 3144/3144 [45:16<00:00,  1.16batch/s, loss=2.96]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 2 with loss 9320.7535\nEpoch [2/30], Loss: 2.9646\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30: 100%|██████████| 3144/3144 [45:38<00:00,  1.15batch/s, loss=2.96]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 3 with loss 9320.2036\nEpoch [3/30], Loss: 2.9644\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30: 100%|██████████| 3144/3144 [45:43<00:00,  1.15batch/s, loss=2.96]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 4 with loss 9319.6337\nEpoch [4/30], Loss: 2.9643\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30: 100%|██████████| 3144/3144 [45:33<00:00,  1.15batch/s, loss=2.96]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 5 with loss 9319.3477\nEpoch [5/30], Loss: 2.9642\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30: 100%|██████████| 3144/3144 [45:37<00:00,  1.15batch/s, loss=2.96]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 6 with loss 9318.9916\nEpoch [6/30], Loss: 2.9641\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30: 100%|██████████| 3144/3144 [45:43<00:00,  1.15batch/s, loss=2.96]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 7 with loss 9318.5504\nEpoch [7/30], Loss: 2.9639\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30: 100%|██████████| 3144/3144 [45:40<00:00,  1.15batch/s, loss=2.96]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 8 with loss 9317.9665\nEpoch [8/30], Loss: 2.9637\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30: 100%|██████████| 3144/3144 [45:27<00:00,  1.15batch/s, loss=2.96]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 9 with loss 9317.5533\nEpoch [9/30], Loss: 2.9636\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30: 100%|██████████| 3144/3144 [44:58<00:00,  1.17batch/s, loss=2.96]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 10 with loss 9317.3327\nEpoch [10/30], Loss: 2.9635\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30: 100%|██████████| 3144/3144 [45:02<00:00,  1.16batch/s, loss=2.96]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 11 with loss 9317.2032\nEpoch [11/30], Loss: 2.9635\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/30: 100%|██████████| 3144/3144 [45:01<00:00,  1.16batch/s, loss=2.96]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at epoch 12 with loss 9317.1645\nEpoch [12/30], Loss: 2.9635\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/30:   2%|▏         | 78/3144 [01:07<44:29,  1.15batch/s, loss=2.97]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Save and Load the Model","metadata":{}},{"cell_type":"code","source":"# # Save the model\n# torch.save(model.state_dict(), 'las_model.pth')\n\n# # Load the model\n# model.load_state_dict(torch.load('las_model.pth'))\n# model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T10:14:30.528382Z","iopub.status.idle":"2024-07-03T10:14:30.528689Z","shell.execute_reply.started":"2024-07-03T10:14:30.528533Z","shell.execute_reply":"2024-07-03T10:14:30.528546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model\nload_checkpoint('/kaggle/working/checkpoints/best_model.pth', model, optimizer)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T19:49:10.154190Z","iopub.execute_input":"2024-07-03T19:49:10.154918Z","iopub.status.idle":"2024-07-03T19:49:10.264189Z","shell.execute_reply.started":"2024-07-03T19:49:10.154886Z","shell.execute_reply":"2024-07-03T19:49:10.263168Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Loaded checkpoint from '/kaggle/working/checkpoints/best_model.pth' (epoch 12, loss 9317.1645)\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"LASModel(\n  (listener): Listener(\n    (layers): ModuleList(\n      (0): LSTM(60, 128, batch_first=True, bidirectional=True)\n      (1-9): 9 x LSTM(256, 128, batch_first=True, bidirectional=True)\n    )\n  )\n  (attender): Attender(\n    (attn): Linear(in_features=384, out_features=128, bias=True)\n    (v): Linear(in_features=128, out_features=1, bias=False)\n  )\n  (speller): Speller(\n    (lstm): LSTM(384, 128, num_layers=10, batch_first=True)\n    (fc): Linear(in_features=128, out_features=52, bias=True)\n    (embedding): Embedding(52, 128)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference (Prediction)","metadata":{}},{"cell_type":"code","source":"def infer_from_folder(model, folder_path, vocab, output_csv_path, max_length=100):\n    \"\"\"\n    Perform inference on all .wav files in a folder and save the transcriptions to a CSV file.\n\n    Parameters:\n        model (LASModel): Trained LAS model.\n        folder_path (str): Path to the folder containing .wav files.\n        vocab (dict): Vocabulary dictionary mapping characters to indices.\n        output_csv_path (str): Path to save the output CSV file.\n        max_length (int): Maximum length of the output sequence.\n\n    Returns:\n        None\n    \"\"\"\n    model.eval()\n    results = []\n    wav_files = [f for f in os.listdir(folder_path) if f.endswith('.wav')]\n\n    # Mapping from indices to characters\n    index_to_char = {idx: char for char, idx in vocab.items()}\n\n    with torch.no_grad():\n        for wav_file in tqdm(wav_files, desc='Inference', unit='file'):\n            audio_id = os.path.splitext(wav_file)[0]  # Remove '.wav' extension to get audio ID\n            audio_path = os.path.join(folder_path, wav_file)\n\n            # Load audio\n            audio_waveform, sample_rate = torchaudio.load(audio_path)\n\n            # Convert to Mel spectrogram\n            mel_transform = torchaudio.transforms.MelSpectrogram(n_mels=feature_dim,sample_rate=sample_rate)\n            mel_features = mel_transform(audio_waveform).squeeze(0).transpose(0, 1)\n\n            inputs = torch.tensor(mel_features, dtype=torch.float32).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\n\n            # Perform inference\n            batch_size = 1\n            encoder_outputs = model.listener(inputs)\n            outputs = torch.zeros(batch_size, max_length, len(vocab)).to(inputs.device)\n\n            hidden = torch.zeros(model.speller.lstm.num_layers, batch_size, model.speller.lstm.hidden_size).to(inputs.device)\n            cell = torch.zeros(model.speller.lstm.num_layers, batch_size, model.speller.lstm.hidden_size).to(inputs.device)\n            previous_char = torch.zeros(batch_size, dtype=torch.long).to(inputs.device)  # Start token\n\n            for t in range(1, max_length):\n                context, _ = model.attender(encoder_outputs, hidden[-1])\n                output, hidden, cell = model.speller(context, hidden, cell, previous_char)\n                outputs[:, t, :] = output\n                previous_char = output.argmax(1)\n\n            predicted_indices = outputs.argmax(2).cpu().numpy().squeeze()\n\n            # Convert indices to characters\n            predicted_transcript = ''.join([index_to_char[idx] for idx in predicted_indices if idx != 0])\n            results.append((audio_id, predicted_transcript))\n\n    # Convert the results to a DataFrame and save to CSV\n    df = pd.DataFrame(results, columns=['audio', 'transcript'])\n    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n    print(f\"Transcriptions saved to {output_csv_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-03T19:28:25.059362Z","iopub.execute_input":"2024-07-03T19:28:25.059721Z","iopub.status.idle":"2024-07-03T19:28:25.074913Z","shell.execute_reply.started":"2024-07-03T19:28:25.059692Z","shell.execute_reply":"2024-07-03T19:28:25.073857Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Example usage\n# Assuming you have a trained model and a vocabulary dictionary `vocab`\nfolder_path = '/kaggle/input/test-data/test/'\ninfer_from_folder(model, folder_path, speech_dataset.vocab, 'transcriptions.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T19:39:53.462786Z","iopub.execute_input":"2024-07-03T19:39:53.463166Z","iopub.status.idle":"2024-07-03T19:44:36.466880Z","shell.execute_reply.started":"2024-07-03T19:39:53.463136Z","shell.execute_reply":"2024-07-03T19:44:36.465993Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"Inference:   0%|          | 0/1726 [00:00<?, ?file/s]/tmp/ipykernel_337/1762800947.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  inputs = torch.tensor(mel_features, dtype=torch.float32).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\nInference: 100%|██████████| 1726/1726 [04:42<00:00,  6.10file/s]","output_type":"stream"},{"name":"stdout","text":"Transcriptions saved to transcriptions.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"%ls","metadata":{"execution":{"iopub.status.busy":"2024-07-03T19:44:36.468687Z","iopub.execute_input":"2024-07-03T19:44:36.469311Z","iopub.status.idle":"2024-07-03T19:44:37.429645Z","shell.execute_reply.started":"2024-07-03T19:44:36.469277Z","shell.execute_reply":"2024-07-03T19:44:37.428574Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  las_model.pth  transcriptions.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'transcriptions.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T19:45:41.746256Z","iopub.execute_input":"2024-07-03T19:45:41.747001Z","iopub.status.idle":"2024-07-03T19:45:41.753913Z","shell.execute_reply.started":"2024-07-03T19:45:41.746964Z","shell.execute_reply":"2024-07-03T19:45:41.752840Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/transcriptions.csv","text/html":"<a href='transcriptions.csv' target='_blank'>transcriptions.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import FileLink\ndf = pd.read_csv('/kaggle/working/transcriptions.csv')\ncsv_file = '/kaggle/working/transcriptions.csv'\ndf.to_csv(csv_file, index=False)\nprint(\"CSV file 'df_train_default' exported successfully.\")\nFileLink(csv_file)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T19:39:41.565043Z","iopub.execute_input":"2024-07-03T19:39:41.566062Z","iopub.status.idle":"2024-07-03T19:39:42.292556Z","shell.execute_reply.started":"2024-07-03T19:39:41.566027Z","shell.execute_reply":"2024-07-03T19:39:42.291151Z"},"trusted":true},"execution_count":47,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileLink\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/transcriptions.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/transcriptions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(csv_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n","File \u001b[0;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"],"ename":"EmptyDataError","evalue":"No columns to parse from file","output_type":"error"}]},{"cell_type":"code","source":"def infer(model, audio_path, vocab, max_length=100):\n    model.eval()\n    with torch.no_grad():\n        audio = speech_dataset._load_audio(audio_path)\n        mel_features = speech_dataset._extract_features(audio)\n        inputs = torch.tensor(mel_features, dtype=torch.float32).unsqueeze(0).to('cuda')\n\n        batch_size = 1\n        encoder_outputs = model.listener(inputs)\n        outputs = torch.zeros(batch_size, max_length, vocab_size).to(inputs.device)\n\n        hidden = torch.zeros(model.speller.lstm.num_layers, batch_size, model.speller.lstm.hidden_size).to(inputs.device)\n        cell = torch.zeros(model.speller.lstm.num_layers, batch_size, model.speller.lstm.hidden_size).to(inputs.device)\n        previous_char = torch.zeros(batch_size, dtype=torch.long).to(inputs.device)  # Start token\n\n        for t in range(1, max_length):\n            context, _ = model.attender(encoder_outputs, hidden[-1])\n            output, hidden, cell = model.speller(context, hidden, cell, previous_char)\n            outputs[:, t, :] = output\n\n            previous_char = output.argmax(1)\n\n        predicted_indices = outputs.argmax(2).squeeze().cpu().numpy()\n        index_to_char = {idx: char for char, idx in vocab.items()}\n        predicted_transcript = ''.join([index_to_char[idx] for idx in predicted_indices if idx != 0])\n        return predicted_transcript","metadata":{"execution":{"iopub.status.busy":"2024-07-03T19:34:48.685349Z","iopub.execute_input":"2024-07-03T19:34:48.685726Z","iopub.status.idle":"2024-07-03T19:34:48.696346Z","shell.execute_reply.started":"2024-07-03T19:34:48.685695Z","shell.execute_reply":"2024-07-03T19:34:48.695453Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Example usage\n# audio_path = '/kaggle/input/arabic-asr/train/train_sample_1.wav'\naudio_path = '/kaggle/input/test-data/test/test_sample_1000_clean.wav'\npredicted_transcript = infer(model, audio_path, speech_dataset.vocab)\nprint(predicted_transcript)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T19:49:21.074991Z","iopub.execute_input":"2024-07-03T19:49:21.075895Z","iopub.status.idle":"2024-07-03T19:49:21.240239Z","shell.execute_reply.started":"2024-07-03T19:49:21.075856Z","shell.execute_reply":"2024-07-03T19:49:21.239390Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"ل                                                                                                  \n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_337/3024956319.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  inputs = torch.tensor(mel_features, dtype=torch.float32).unsqueeze(0).to('cuda')\n","output_type":"stream"}]}]}